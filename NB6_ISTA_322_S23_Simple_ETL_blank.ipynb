{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alostmathematician/ISTA-366/blob/main/NB6_ISTA_322_S23_Simple_ETL_blank.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E7t6TrEQ6SBR"
      },
      "source": [
        "# Wrapping up ETL with the L\n",
        "\n",
        "The goal of this week's lesson is to teach you the process of uploading data to a database (You already followed a lesson to create a Postgres database on AWS). Now it's time to load data to it.\n",
        "\n",
        "Before we do that though, we're going to extract data and transform it into a more useful analytical format. Thus, we'll be doing a full ETL pipeline today!\n",
        "\n",
        "The data that we'll be extracting contains daily numbers of Covid-19 positive cases and deaths by each county and state in the US. Most dashboards or reporting metrics are going to want values 'rolled up' into a higher level. We're going to do this in two ways:\n",
        "\n",
        "* We're going to want aggregate numbers by state. Hearing about high rates in some specific county might matter for some people, but most sources will want state-level numbers.\n",
        "\n",
        "* We're also going to want to rollup our data into weekly aggregates vs. daily numbers. Daily numbers are problematic as some sources only report on specific days of the week. For example, hospitals might not update their stats on the weekends. Similarly U Arizona doesn't test on the weekends. This means that daily numbers aren't an accurate reflection of the overall trend, while weekly numbers are.\n",
        "\n",
        "The last thing we'll do is some wrangling to make a unique primary key for each row.\n",
        "\n",
        "Once that is done *then* we can upload the table to our database."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Auw-NIWr92Nn"
      },
      "source": [
        "## Extracting our data\n",
        "\n",
        "The data has been stored on my Google Drive.\n",
        "\n",
        "We can bring the data in via our usual `pd.read_csv()`. Then we'll do a quick exploration to figure out what we need to do as far as transforms."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c8J9IVB99Cw7"
      },
      "source": [
        "# Get pandas\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iikJFXUt9EVJ"
      },
      "source": [
        "# We'll call our data 'covid'\n",
        "covid = pd.read_csv('https://docs.google.com/spreadsheets/d/1KhRW9lU_mm9svjy2T2u2Sx6UwTGcOfXx1jb1xJoxLpQ/gviz/tq?tqx=out:csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rai4DP4P-fm-"
      },
      "source": [
        "### Quick explore\n",
        "\n",
        "Let's look at the head, a describe, and shape."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "adY6xrly-k4L"
      },
      "source": [
        "# head\n",
        "covid.tail()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CeTPf8wr-ozt"
      },
      "source": [
        "# describe\n",
        "covid.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_rjxQ1zK-rrU"
      },
      "source": [
        "# dtypes\n",
        "covid.dtypes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EQPojiU_-ubT"
      },
      "source": [
        "# shape\n",
        "covid.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9XqzzOsW-xMW"
      },
      "source": [
        "OK, we can see a few things. We have six columns. Our shape tells us that we have over 600,000 rows. Our dtypes call indicates that our `date` column isn't currently a datetime. So if we're going to want to roll up by week then we'll need to convert that."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mWUoi-f7EtFC"
      },
      "source": [
        "## Transforming our data\n",
        "\n",
        "Time to get our transform going. We're going to do the following steps:\n",
        "\n",
        "* Make `date` a datetime\n",
        "* Aggregate our data by week and state and count up the number of cases/deaths\n",
        "* Make a unique key that's needed for our database"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-6RS9s0MJQlo"
      },
      "source": [
        "### Make our datetime\n",
        "\n",
        "You know how this is done. Just convert `date` in place."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IKMJfy0s_H8h"
      },
      "source": [
        "# Convert date to datetime\n",
        "covid['date'] = pd.to_datetime(covid['date'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "923OwRl2_f-n"
      },
      "source": [
        "# Can we call a method on it to make sure it returns a datetime?\n",
        "covid['date'].min()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PEaobP9sJpZL"
      },
      "source": [
        "### Groupby and aggregate\n",
        "\n",
        "Time to groupby. We're going to group by two levels. The first is `state`, which is easy. The second is that we want to group by the calendar week. You could go and do something where you extract days and then have some rolling function that counts in groups of seven. But, pandas can actually group by calendar week if we use the `Grouper()` function!\n",
        "\n",
        "To use `pd.Grouper()` you give it two arguments. The first is the key, which in this case is the `date` column. The second is the frequency that you want to group. We want to group on weeks so we can just specify `freq = 'w'`. The `W` tells it to group by calendar week. You can even specify what day you want the week to start on by adding a dash and a day abbreviation. So `W-MON` will tell it to group by weeks where weeks start on Monday.\n",
        "\n",
        "For our `.agg()` we just want to sum up `daily_cases` and `daily_death` which will give us the weekly cases and deaths.\n",
        "\n",
        "OK, let's do it and call the resulting object `covid_grouped`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KWaBhwKo9upN"
      },
      "source": [
        "# Test out using grouper. Play around with different freq abbreviations and observe the result\n",
        "covid_grouped_test = covid.groupby([covid['state'], pd.Grouper(key = 'date', freq='...')]).agg({'daily_cases': ['sum'], 'daily_deaths': ['sum']})\n",
        "covid_grouped_test.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qnd20-CPJlOh"
      },
      "source": [
        "# Make covid_grouped for real\n",
        "covid_grouped = covid.groupby([covid['state'], pd.Grouper(key = 'date', freq = 'W-MON')]).agg({'daily_cases' : ['sum'], 'daily_deaths': ['sum']})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NV-_pXOqMaa3"
      },
      "source": [
        "# Check the head of covid_grouped\n",
        "covid_grouped.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0eWIENbDMd5X"
      },
      "source": [
        "Right now see how the state and date columns are in bold. That means they got assigned as indexes and not columns. Let's reset the index to bring them back as columns. We want them as columns for date manipulation and key generation later."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oNci2u8I_mMB"
      },
      "source": [
        "# Reset index and check head\n",
        "covid_grouped = covid_grouped.reset_index()\n",
        "covid_grouped.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ihyh0dvM3_7"
      },
      "source": [
        "Let's also rename our columns. We'll leave the first two as 'state' and 'date', but change the last two to 'weekly_cases' and 'weekly_deaths'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p0Nn8IkUG3oV"
      },
      "source": [
        "# rename to 'state', 'date', 'weekly_cases', 'weekly_deaths'\n",
        "covid_grouped.columns = ['state', 'date', 'weekly_cases', 'weekly_deaths']\n",
        "covid_grouped.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "inCqyvFDNFvY"
      },
      "source": [
        "### A quick validation of our data\n",
        "\n",
        "Let's take a minute and make sure our data makes sense. We'll extract out data from just Arizona and then do a quick barplot of the number of weekly cases in AZ."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QsfR5M_wGoDg"
      },
      "source": [
        "# Extract just AZ observations\n",
        "az_data = covid_grouped[covid_grouped['state'] == 'Arizona']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#For local machine you may need to install seaborn\n",
        "!pip install seaborn"
      ],
      "metadata": {
        "id": "24Qhm-PnzQTF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZcmpP2BpGvdh"
      },
      "source": [
        "# Make barplot using seaborn\n",
        "# The x-axis is a mess but data look right based on what we know about Covid in AZ\n",
        "import seaborn as sns\n",
        "sns.barplot(x = 'date', y = 'weekly_cases', data = az_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ct9XQL2yYOIL"
      },
      "source": [
        "## Transforms - Making a key\n",
        "\n",
        "OK, our `covid_grouped` dataframe is now rolled up into a format that we want. But since we're going to push to a database we need to also make a key. A **primary key** is a unique identifying value for a row of the table. Such a key must not be duplicated in any of the rows (it wouldn't be unique if it was!).\n",
        "\n",
        "So, we need to make a key. What should we use? There isn't a firm rule here, but ideally you want something that is easy to generate. We could imagine making a table of a bunch of random values and selecting from that. The only issue is that if we want to add more data, every time you do you'd need to make sure the key you're giving a row hasn't already been used. You could also imagine making the ID just from a squence, but again if you update you need to know where the sequence ends.\n",
        "\n",
        "Instead of either of those two options we can do something simpler. Let's just take the date in YYYYMMDD format and then add on a code for the state. Any updates obviously won't duplicate dates. Adding on the state code ensures that each date is then unique as no states will have the same state code.\n",
        "\n",
        "To make the key we'll do the following steps:\n",
        "* Make a column called `week_start` that takes the date the grouped week began on and converts it back into a string.\n",
        "* We'll remove the dashes from the current YYYY-MM-DD format\n",
        "* Make numeric values for each state in the dataset\n",
        "* Add those state numeric values back to the date string"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QuH97sTHagl-"
      },
      "source": [
        "### Making our date string\n",
        "\n",
        "First we'll use our basic wrangling skills to take our date column and convert it back to a string with `.astype(str)`. We'll call the new column 'week_start'."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1AQYUOnrIWMY"
      },
      "source": [
        "# Convert to string and check\n",
        "covid_grouped['week_start'] = covid_grouped['date'].astype(str)\n",
        "covid_grouped.dtypes # can see that week_start is an object"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CXNl5FLta3AS"
      },
      "source": [
        "Removing symbols from strings is really common. Luckily it's also quite easy here and we don't need regular expression. `.replace()` allows you to just specify what symbol you want it to remove first, in this case the \"-\", and then what you want to replace it with, in this case nothing.\n",
        "\n",
        "Let's do this operation on 'week_start'."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cX4q1iTfw1wn"
      },
      "source": [
        "covid_grouped.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "viFlaqTeIhjZ"
      },
      "source": [
        "# Remove dash from week start\n",
        "covid_grouped['week_start'] = covid_grouped['week_start'].str.replace('-', '')\n",
        "covid_grouped.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3alOQBFKbTkX"
      },
      "source": [
        "### Making our state names into numeric factors\n",
        "\n",
        "We want to assign each unique level in the 'state' column it's own numeric ID value. Pandas has a function called `factorize()` that does just that. As an argument you feed it a column that has multiple unique characters, and it'll return a unique numeric value for each. Here's an example on some test data\n",
        "\n",
        "```\n",
        ">>> states = ['washington', 'washington', 'arizona', 'california']\n",
        ">>> pd.factorize(states)\n",
        "(array([0, 0, 1, 2]),\n",
        " array(['washington', 'arizona', 'california'], dtype=object))\n",
        "```\n",
        "You can see that `pd.factorize()` took 'states` and gave the unique values a corresponding numeric value starting with zero. Test it out below if you want.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kXr2Tjnlb04f"
      },
      "source": [
        "# If you want to test it out\n",
        "states = ['washington', 'washington', 'arizona', 'california']\n",
        "pd.factorize(states)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D6pAqcRhcjmn"
      },
      "source": [
        "Let's apply factorize to create a new column called 'state_id'. Note how above `factorize()` returned two arrays. The first had the ID, the second the original level. We'll need to call just that first array with `[0]` as that's all we want in the new column.\n",
        "\n",
        "I'm also going to convert the numeric straight to a string using `.astype(str)` again. This will save us a step when we add the `state_id` string to the `week_start` string."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZH9eQtOrcjOe"
      },
      "source": [
        "covid_grouped['state_id'] = pd.factorize(covid_grouped['state'])[0].astype(str)\n",
        "covid_grouped # check it"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "As7ZiyGweXmt"
      },
      "source": [
        "Great. We can see we have our 'state_id' column now. Let's join that to the front of 'week_start' so this way our key format is IDYYYYMMDD where ID at the start of the string is the state ID.\n",
        "\n",
        "If you want to merge two strings in different columns you can literally just add them together. So let's add the 'state_id' column to the 'week_start' column. Let's create a new column called 'wk_st_id' for \"week state id.\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZIz1ENUlPQOA"
      },
      "source": [
        "# Add strings\n",
        "covid_grouped['wk_st_id'] = covid_grouped['state_id'] + covid_grouped['week_start']\n",
        "covid_grouped #check"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xparPAPxf2Cf"
      },
      "source": [
        "## Making our table of state keys\n",
        "\n",
        "Now that we have our encoded key for our covid_grouped table we need to do two final things. First, let's make a second table that contains the relationship between the state_id and the state. This way people can make sql queries on the covid data using the state name if they wanted.\n",
        "\n",
        "After that we'll drop the unnecessary columns from `covid_grouped`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zNauOlS2gVbx"
      },
      "source": [
        "Making our 'states' table is easy. Just select the two columns of 'state_id' and 'state' and use `drop_duplicates() to get just unique values. This way we'll have each state only once. Call the resulting dataframe 'states'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cusIG-KbR8j7"
      },
      "source": [
        "# Make states by selecting columns and dropping duplicates\n",
        "states = covid_grouped[['state_id', 'state']].drop_duplicates()\n",
        "states.head() # check"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bANdkYFpgsUJ"
      },
      "source": [
        "Now we'll drop 'week_start' and 'state' from `covid_grouped` as those columns are redundant. The week start is just the 'date' column which we'll keep. And we have a 'state_id' column so we don't need the state name.\n",
        "\n",
        "`.drop()` is useful here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ClK9mllLxR-"
      },
      "source": [
        "# Drop columns\n",
        "covid_grouped = covid_grouped.drop(columns=['week_start', 'state'])\n",
        "covid_grouped.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i2Ah2aLIpbkJ"
      },
      "source": [
        "## Pushing to our/your SQL database\n",
        "\n",
        "Alright, let's go and push our two tables to the database that we set up earlier in the week.\n",
        "\n",
        "Pushing data to a database has two steps.\n",
        "\n",
        "1. We define our table columns and the datatypes within in\n",
        "2. We upload data into the columns that we created\n",
        "3. We commit the changes\n",
        "\n",
        "Steps 1 and 2 are done using SQL syntax in a similar structure as we used to make queries. We'll write our commands and then execute them using a cursor just like before. Then we'll commit our changes and close our cursor."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IRnzVNEhfwqu"
      },
      "source": [
        "### Import SQL functions\n",
        "\n",
        "First, go and run the functions we've been using. **Update** the `get_conn_cur()` function so that it connects to your database!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cL_VSnhjfKnu"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yOx5P_uEqoiE"
      },
      "source": [
        "#If you are using AWS, use this cell\n",
        "import psycopg2\n",
        "def get_conn_cur():\n",
        " # UPDATE WITH YOUR INFO!\n",
        "\n",
        " conn = psycopg2.connect(\n",
        "    host=\"YOUR DB HOST INFO HERE\",\n",
        "    database=\"YOUR DB NAME HERE\",\n",
        "    user=\"YOUR DB USER HERE\",\n",
        "    password=\"YOUR DB PASSWORD HERE\",\n",
        "    port='YOUR DB PORT HERE'\n",
        "    )\n",
        "\n",
        " cur = conn.cursor()\n",
        " return(conn, cur)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#If you using local MySQL use this cell. (Note: if you are using local MySQL you must run this notebook on your local Jupyter, it won't work on Google Colab)\n",
        "import mysql.connector\n",
        "def get_conn_cur():\n",
        "    cnx = mysql.connector.connect(user='root', password='',\n",
        "          host='127.0.0.1',\n",
        "          database='YOUR DB NAME HERE');\n",
        "    return (cnx, cnx.cursor())"
      ],
      "metadata": {
        "id": "bhQelKRsa3Sq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GMR4jkCueZnd"
      },
      "source": [
        "# Run or old functions in case we want to check our data!\n",
        "\n",
        "# run_query function\n",
        "def run_query(query_string):\n",
        "\n",
        " conn, cur = get_conn_cur() # get connection and cursor\n",
        "\n",
        " cur.execute(query_string) # executing string as before\n",
        "\n",
        " my_data = cur.fetchall() # fetch query data as before\n",
        "\n",
        " # here we're extracting the 0th element for each item in cur.description\n",
        " colnames = [desc[0] for desc in cur.description]\n",
        "\n",
        " cur.close() # close\n",
        " conn.close() # close\n",
        "\n",
        " return(colnames, my_data) # return column names AND data\n",
        "\n",
        "# Column name function for checking out what's in a table\n",
        "def get_column_names(table_name): # arguement of table_name\n",
        " conn, cur = get_conn_cur() # get connection and cursor\n",
        "\n",
        " # Now select column names while inserting the table name into the WERE\n",
        " column_name_query = \"\"\"SELECT column_name FROM information_schema.columns\n",
        "    WHERE table_name = '%s' \"\"\" %table_name\n",
        "\n",
        " cur.execute(column_name_query) # exectue\n",
        " my_data = cur.fetchall() # store\n",
        "\n",
        " cur.close() # close\n",
        " conn.close() # close\n",
        "\n",
        " return(my_data) # return\n",
        "\n",
        "# Check table_names\n",
        "def get_table_names():\n",
        "  conn, cur = get_conn_cur() # get connection and cursor\n",
        "\n",
        "  # query to get table names\n",
        "  table_name_query = \"\"\"SELECT table_name FROM information_schema.tables\n",
        "       WHERE table_schema = 'public' \"\"\"\n",
        "\n",
        "  cur.execute(table_name_query) # execute\n",
        "  my_data = cur.fetchall() # fetch results\n",
        "\n",
        "  cur.close() #close cursor\n",
        "  conn.close() # close connection\n",
        "\n",
        "  return(my_data) # return your fetched results\n",
        "\n",
        "# make sql_head function\n",
        "def sql_head(table_name):\n",
        " conn, cur = get_conn_cur() # get connection and cursor\n",
        "\n",
        " # Now select column names while inserting the table name into the WERE\n",
        " head_query = \"\"\"SELECT * FROM %s LIMIT 5; \"\"\" %table_name\n",
        "\n",
        " cur.execute(head_query) # exectue\n",
        " colnames = [desc[0] for desc in cur.description] # get column names\n",
        " my_data = cur.fetchall() # store first five rows\n",
        "\n",
        " cur.close() # close\n",
        " conn.close() # close\n",
        "\n",
        " df = pd.DataFrame(data = my_data, columns = colnames) # make into df\n",
        "\n",
        " return(df) # return\n",
        "\n",
        "# drop a table from your rdb (if you try to create a table that already exists, it'll throw an error)\n",
        "def my_drop_table(tab_name):\n",
        "  conn, cur = get_conn_cur()\n",
        "  tq = \"\"\"DROP TABLE IF EXISTS %s CASCADE;\"\"\" %tab_name\n",
        "  cur.execute(tq)\n",
        "  conn.commit()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lNpGP1VYhPOK"
      },
      "source": [
        "## Create weekly table in database\n",
        "\n",
        "Let's go and make a table called 'weekly' that we'll insert our main 'covid_grouped' dataset into'\n",
        "\n",
        "The main call to make a table is `CREATE TABLE new_table_name ()`. Inside the parentheses you then give it your column name, your datatype, and if you can have null values in there. We want to make five columns. The first one we want to be our 'wk_st_id' which we'll set as an integer and a primary key.\n",
        "\n",
        "I've written the query for you.\n",
        "\n",
        "**NOTE** - `VARCHAR(255)` is the way to specify a character column. Inside the parentheses is the max length allowed. 'state_id' doesn't need to be a character, but I specified it as one to demonstrate some variety. It's also how we already had the datatype specified in our 'covid_grouped' dataframe. We could specify the column as an integer but then we'd have to also change 'state_id' in 'covid_grouped' to an integer so the datatypes being pushed match."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7HXGEgk8hR-7"
      },
      "source": [
        "# drop the table if it already exists (uncomment)\n",
        "# my_drop_table('weekly_stats')\n",
        "\n",
        "# query with wk_st_id as an integer and primary key\n",
        "# date as a timestamp\n",
        "# cases and deaths as intergers\n",
        "# state_id as characters\n",
        "tq = \"\"\"CREATE TABLE weekly_stats (\n",
        "     wk_st_id INT PRIMARY KEY,\n",
        "     date timestamp NOT NULL ,\n",
        "     weekly_cases BIGINT NOT NULL,\n",
        "     weekly_deaths BIGINT NOT NULL,\n",
        "     state_id BIGINT NOT NULL\n",
        "     );\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yYPHdIkevWm-"
      },
      "source": [
        "Now that we have that let's get a connection and cursor object"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tJVkBmFhjD_r"
      },
      "source": [
        "# Get our connection and cursor and assign to conn and cur\n",
        "conn, cur = get_conn_cur()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "swWdW24Ovfmo"
      },
      "source": [
        "To create the table we can just execute the query using `cur.execute()`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r_c5RGs1jIt5"
      },
      "source": [
        "# execute our query we wrote above\n",
        "cur.execute(tq)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sy4lPfVTvslH"
      },
      "source": [
        "And finally just commit the table. Doing so just finalizes everything you did and will make it accessable to other users."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l4KsIlnajURe"
      },
      "source": [
        "# commit\n",
        "conn.commit()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mwoohk_ywEIM"
      },
      "source": [
        "Let's check that our table name and column names are in there using the functions we made in previous lessons."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OQFKS6FajQlQ"
      },
      "source": [
        "# Is the table there?\n",
        "get_table_names()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e6I51tdcjXVU"
      },
      "source": [
        "# All of our column names?\n",
        "get_column_names(table_name='weekly_stats')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mbG0djKZwT2a"
      },
      "source": [
        "### Updating our table\n",
        "\n",
        "I actually made a mistake above. I said the primary key should be an integer, but actually it needs to be something called a big interger. This is because regular integers can only store values between -2147483648 and +2147483647. Our current wk_st_id values get larger than that. So, we need to change the datatype of that column!\n",
        "\n",
        "To do this, or change any datatype for that matter, we write another query. In this case, the main arguments are ALTER TABLE and ALTER COLUMN. The first just points to the table in the database, the second the column in that database. Then you just tell it what datatype you want to use. the general format is as follows:\n",
        "\n",
        "```\n",
        "ALTER TABLE table_name\n",
        " ALTER COLUMN column_to_update TYPE datatype_to_use\n",
        "```\n",
        "\n",
        "I've written the query below. We'll create a cursor and commit everything in one go just like above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lmd9mg9VwQ5J"
      },
      "source": [
        "conn, cur = get_conn_cur()\n",
        "cur = conn.cursor()\n",
        "uq = \"\"\"ALTER TABLE weekly_stats\n",
        "     ALTER COLUMN wk_st_id TYPE BIGINT;\"\"\"\n",
        "cur.execute(uq)\n",
        "conn.commit()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ND85NyTnx2rh"
      },
      "source": [
        "We can run a quick query to get the datatypes. Remember how we called the 'information_schema' before to get column names? We can also get the data types by selecting that information."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XCX-l8HXyB55"
      },
      "source": [
        "conn, cur = get_conn_cur()\n",
        "cur = conn.cursor()\n",
        "uq = \"\"\"SELECT column_name, data_type\n",
        "     FROM information_schema.columns\n",
        "     WHERE table_name = 'weekly_stats';\"\"\"\n",
        "cur.execute(uq)\n",
        "print(cur.fetchall())\n",
        "conn.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IBrVCN4l0T7n"
      },
      "source": [
        "Great, we can see all of our column names and corresponding datatypes. We can see that 'wk_st_id' is now a 'bigint.' This brings up an important point when designing a database. You need to be sure to make your columns are big enough to store approved values now, but also in the future."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FRcj0loX0ksb"
      },
      "source": [
        "## Make states table\n",
        "\n",
        "Can you go and make a second table called 'states'? We're going to want to take that 'states' dataframe and add it to our database. This one should have only two columns, both with character datatypes like above. The primary key should be 'state_id'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N4WActB00-Qj"
      },
      "source": [
        "states.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bxqtaq5f079e"
      },
      "source": [
        "# Make your states table\n",
        "conn, cur = get_conn_cur()\n",
        "cur = conn.cursor()\n",
        "tq = \"\"\"CREATE TABLE ... (\n",
        "     ...,\n",
        "     ...\n",
        "     );\"\"\"\n",
        "cur.execute(tq)\n",
        "conn.commit()\n",
        "conn.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d680EFe_1WoQ"
      },
      "source": [
        "# check that the table is tere\n",
        "get_table_names()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I8bs01A01a3-"
      },
      "source": [
        "# Check that the column names are there\n",
        "get_column_names(table_name='states')\n",
        "sql_head('states')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4N288c6_1kvW"
      },
      "source": [
        "# Might as well check the datatypes\n",
        "conn, cur = get_conn_cur()\n",
        "cur = conn.cursor()\n",
        "uq = \"\"\"SELECT column_name, data_type\n",
        "     FROM information_schema.columns\n",
        "     WHERE table_name = 'states';\"\"\"\n",
        "cur.execute(uq)\n",
        "print(cur.fetchall())\n",
        "conn.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4FUvTOAr2gMN"
      },
      "source": [
        "## Putting information in the database\n",
        "\n",
        "Now we have a couple empty tables sitting in our database waiting for data. As you can imagine, the general process is an extension of what we did above. We write a query that specifies the table and columns we want to insert data into, and then what data we want to put in there.\n",
        "\n",
        "The main argument to do this is INSERT into followed by VALUES. The generalized string format is:\n",
        "\n",
        "```\n",
        "\"INSERT INTO table_name(column_names) VALUES (values_for_each_column)\"\n",
        "```\n",
        "\n",
        "This process is typically done row-wise meaning you insert one row at a time. For example, this string would add just a single row of data into our 'weekly_stats' table:\n",
        "\n",
        "```\n",
        "\"INSERT INTO weekly_stats(wk_st_id,date,weekly_cases,weekly_deaths,state_id\n",
        " VALUES(0020200316, 2020-03-16 00:00:00, 19, 0, 00)\"\n",
        "```\n",
        "\n",
        "**OK, but does this mean we have to write and execute a query for each and every row in our dataframe?** No, of course not. While you could update it a single row at a time using `cur.execute()` and loop through every row of your dataset, that would be super slow as you'd be generating a cursor, executing, and closing each and every time.\n",
        "\n",
        "Instead you can use `cur.executemany()`. This allows you to, well, execute many actions at once. It is a bit more involved in the string you'll have to write as you'll have to use string formatting (`%s`) to generate the string for each row. But, this is simpler and MUCH MUCH faster than using a loop with single operations.\n",
        "\n",
        "There are some other bits to make this work, but we'll cover them as we go. Let's get started!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OEY-gwvm5qyE"
      },
      "source": [
        "First, we need to reorder our columns in the dataframe to match the column order of the database."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UnOa3j585vhS"
      },
      "source": [
        "# What's the order of our weekly_stats column?\n",
        "get_column_names(table_name='weekly_stats')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tpPYmBbcjkzU"
      },
      "source": [
        "# Reorder covid_grouped to match\n",
        "covid_grouped = covid_grouped[['wk_st_id', 'date', 'weekly_cases', 'weekly_deaths', 'state_id']]\n",
        "covid_grouped.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LcXTpE_o5-ni"
      },
      "source": [
        "OK, first think to point out. SQL wants to receive the data as a tuple. Each row of ours is currently a row in a dataframe. So we need to convert each row to a tuple. We can use numpy to do this. Below I write a list comprehension to do this. List comprehensions are essentially simplified for loops.\n",
        "\n",
        "The first part is the action. It's saying 'make a tuple with x'. The second part is just saying the variable name in the sequence. The third part is the list of data. In this case we're making a list of data by converting our dataframe to a numpy array. Play around making a cell and running `covid_grouped.to_numpy()` if you want to see more specifically what this is doing. I'm going to just run the list comprehension for now."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f4XoGuBCf5we"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "covid_np = covid_grouped.to_numpy();\n",
        "#Mysql does not understand the timestamp from python so we can convert the timestamp to string.\n",
        "covid_np[:,1] = np.vectorize(lambda x: str(x))(covid_np[:,1])\n",
        "data_tups = [tuple(x) for x in covid_np]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K-fbhcpj7Tl2"
      },
      "source": [
        "What's in `data_tups`? Let's look at the 3rd position."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XdIPyIwelTbn"
      },
      "source": [
        "# Check out an example\n",
        "data_tups[2]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OR5mZaZC7e5r"
      },
      "source": [
        "Great, so now that's in a format that's ready for VALUES part of our INSERT string. Now we can write our insert string that will take those values.\n",
        "\n",
        "Remember from before that %s allows you to insert a value into a string. So if we did something like `'I need %s points to pass' % points_needed` it would insert whatever 'points_needed' is defined as into that string."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZkabmU7R7eav"
      },
      "source": [
        "# and example\n",
        "points_needed = 20\n",
        "'I need %s points to pass' % points_needed"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IzGucyz19nSH"
      },
      "source": [
        "What we need to do for our query is insert one of our tuples from 'data_tups' into VALUES. WE have five values in each tuple, so we need to just write '%s inside that string five times. Let's write our insert string."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0IJ-OYeZgxXx"
      },
      "source": [
        "iq = \"\"\"INSERT INTO weekly_stats(wk_st_id,date,weekly_cases,weekly_deaths,state_id) VALUES(%s, %s, %s, %s, %s);\"\"\"\n",
        "iq"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PrsM7GmG-D-g"
      },
      "source": [
        "You can test if it's working by trying to add a tuple to your insert string."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mLIO1aDwmBYU"
      },
      "source": [
        "# Does it work?\n",
        "iq % data_tups[4]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QJ4evTb8-Nuz"
      },
      "source": [
        "### Finally pushing data to the database\n",
        "\n",
        "Now we can finally use `executemany() to push our database. This'll take two arguments. The first is the query string, the section is our tuples. It'll go and make an insert string for each and every tuple and therefore insert each row. We still need to open a connection, make a cursor, close, and commit.\n",
        "\n",
        "**NOTE** this will take a minute or so!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7pkhXYuZkbk5"
      },
      "source": [
        "# Upload data\n",
        "conn, cur = get_conn_cur()\n",
        "cur.executemany(iq, data_tups)\n",
        "conn.commit()\n",
        "conn.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BRoOz_dn-sT-"
      },
      "source": [
        "Let's give it a check with a simple query"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tE9jjalHqPuQ"
      },
      "source": [
        "sq = \"\"\" SELECT COUNT(DISTINCT(wk_st_id)) FROM weekly_stats\n",
        "     LIMIT 5;\"\"\"\n",
        "run_query(sq)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MUMozTaSADlx"
      },
      "source": [
        "**SUCCESS**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1G_eEaAvAB23"
      },
      "source": [
        "### Add data to states table\n",
        "\n",
        "Wonderful! Can you now go and populate the 'states' table?\n",
        "\n",
        "You'll need to make tuples and modify the insert statement to take only two arguments along with the proper table name and column names"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AhBhhkn9AVo0"
      },
      "source": [
        "import numpy as np\n",
        "..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0RO8GO2cAd2A"
      },
      "source": [
        "get_column_names(table_name='states')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "775P2OKpAlOZ"
      },
      "source": [
        "# new insert query\n",
        "iq = \"\"\"...;\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tiDQzrOtAW9T"
      },
      "source": [
        "# Upload data\n",
        "..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hnVrRQeXq1i6"
      },
      "source": [
        "## Run some test queries!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5qfG7_yAmnqT"
      },
      "source": [
        "# Test query to look at the first five rows of states\n",
        "sq = \"\"\" SELECT * FROM states\n",
        "     LIMIT 5;\"\"\"\n",
        "run_query(sq)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6AlVln5PoTKX"
      },
      "source": [
        "# And weekly_stats\n",
        "sq = \"\"\" SELECT * FROM weekly_stats\n",
        "     LIMIT 5;\"\"\"\n",
        "run_query(sq)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O-ptbT6eCHkp"
      },
      "source": [
        "\n",
        "# Do a subquery getting just weekly stats where the state is 'Arizona'\n",
        "sq = \"\"\" SELECT * FROM weekly_stats\n",
        "     WHERE state_id = (SELECT state_id FROM states WHERE state = 'Arizona')\n",
        "     LIMIT 5;\"\"\"\n",
        "run_query(sq)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}