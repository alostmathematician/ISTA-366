{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alostmathematician/ISTA-366/blob/main/%E2%80%9CNB5_ISTA_322_SQL_intermediate_ipynb%E2%80%9D%E7%9A%84%E5%89%AF%E6%9C%AC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZJZAMEiLj4PY"
      },
      "source": [
        "# SQL\n",
        "\n",
        "Woooooo, I know you're all excited for more SQL work!  This week we're going to go deeper into the SQL world and start doing more filtering, aggregations, grouping, joins, and mixes of them all.  \n",
        "\n",
        "You've already been introduced to all these topics and learned how to code them in Python.  A fair question is: why do this in SQL vs. just doing it locally?  Well, two reasons.  For one, your data might just be too darn big to bring in locally, so it's more efficient to do this on the SQL server.  The other might be that even if your data is small it might be more efficient to do the bulk of the work on the SQL server and then bring it in locally.  Doing that would mean less data transfer and ultimately faster speed.  \n",
        "\n",
        "There are not strict rules regarding what you should do via SQL vs. Python, but I'd say it makes sense to do most of the simple selections, filtering, aggregations via SQL.  If you have to do more complicated work or a lot of exploration, it may pay to bring it in locally and work there.  But again, each situation is different so I'm not going to try and make a rulebook.  Use your best judgement.  \n",
        "\n",
        "What are we going to learn this lesson?  \n",
        "\n",
        "* GROUP BY - We'll first do some GROUP BY operations in order to aggregate some basic information about our movies\n",
        "* HAVING - HAVING is a way to filter on the output of your GROUP BY.  For example, if you want to get the average rating for each movie, but only return those that have an average greater than 3\n",
        "* JOIN - You've seen JOIN before, and now we're going to do them in SQL.  They allow us to bring together different tables into one.  \n",
        "* Subqueries - Subqueries are, well, queries within queries.  You might want to get a query that meets a condition in one table, and then return only values from a different table that match that condition.  It's essentially query inception.  \n",
        "* Odds and ends - We'll also learn some other random bits such as how to do math across columns, rename columns, expand our functions, make some new ones, etc.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T9rhnLVEtQg6"
      },
      "source": [
        "## Libraries and functions\n",
        "We'll start by bringing in our usual libraries.  \n",
        "\n",
        "We're also going to bring in our functions but we're going to modify them a bit to give us more info.  We're also going to make a couple new ones to check what tables are in our database and what column names are within our tables."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WPVtGM-EtwDP"
      },
      "source": [
        "### Connection function\n",
        "\n",
        "This one we'll keep the same!  It just returns a connection called `conn` and a cursor call `cur`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w3WjSImSwD8W"
      },
      "source": [
        "import psycopg2\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LLzFS7fph29H"
      },
      "source": [
        "# Make our connection/cursor function\n",
        "AWS_host_name = \"moviesdb.cp9xqenk8fzu.us-east-1.rds.amazonaws.com\"\n",
        "AWS_dbname = \"moviesdb\"\n",
        "AWS_user_name = \"postgres\"\n",
        "AWS_password = \"ista322moviesdb\"\n",
        "\n",
        "def get_conn_cur(): # define function name and arguments (there aren't any)\n",
        "  # Make a connection\n",
        "  conn = psycopg2.connect(\n",
        "    host=AWS_host_name,\n",
        "    database=AWS_dbname,\n",
        "    user=AWS_user_name,\n",
        "    password=AWS_password,\n",
        "    port='5432')\n",
        "\n",
        "  cur = conn.cursor()   # Make a cursor after\n",
        "\n",
        "  return(conn, cur)   # Return both the connection and the cursor"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wn7HzkzxsIOA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d2d134c9-3425-4bb0-9989-ac3d3fb76f00"
      },
      "source": [
        "conn, cur = get_conn_cur()\n",
        "conn"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<connection object at 0x7cf1f5ba9940; dsn: 'user=postgres password=xxx dbname=moviesdb host=moviesdb.cp9xqenk8fzu.us-east-1.rds.amazonaws.com port=5432', closed: 0>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_uCkaM4culWA"
      },
      "source": [
        "### Table name function\n",
        "\n",
        "Let's make a function that allows us to quickly check what table names are present.  This is actually just the same function we used before to run a query string, but this time we're embedding just the one query string in the function so it only gets the table names.  This way you don't have to rewrite the string and then call it in the old function.\n",
        "\n",
        "As you saw in the last lesson, our database contains something called an information_schema which contains, well, information about our database.  Within that you can get the tables, hence calling `infomation_schema.tables` and selecting just their names.  Feel free to check out the other information in `information_tables` here: https://www.postgresql.org/docs/9.1/information-schema.html!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BlRZmSizwU9k"
      },
      "source": [
        "def get_table_names():\n",
        "  conn, cur = get_conn_cur() # get connection and cursor\n",
        "\n",
        "  # query to get table names\n",
        "  table_name_query = \"\"\"SELECT table_name FROM information_schema.tables\n",
        "       WHERE table_schema = 'public' \"\"\"\n",
        "\n",
        "  cur.execute(table_name_query) # execute\n",
        "  my_data = cur.fetchall() # fetch results\n",
        "\n",
        "  cur.close() #close cursor\n",
        "  conn.close() # close connection\n",
        "\n",
        "  return(my_data) # return your fetched results"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yNApkbBuvv5G"
      },
      "source": [
        "Now we can run just the function `get_table_names()` whenever we want to remember what tables we have.  This is a lot easier and faster than rewriting that query statement each time and running it!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4xeADprZvGPQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33224502-0d5f-4d18-937f-a21816d69af8"
      },
      "source": [
        "# Run it!h\n",
        "get_table_names()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('movies',), ('ratings',), ('movie_sales',)]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JnJeywBVv9Sl"
      },
      "source": [
        "### Column names function\n",
        "\n",
        "Let's also make a function to tell us what the column names are in a given table.  This is a touch tricker as we need to specify the table name as an argument and then insert that inside of a string.  We'll need to do just a touch of string formatting to make it work.  \n",
        "\n",
        "`%s` as a placeholder operator that allows you to pass a specified value into a string.  You specify what value you want to place there by using `%` and then what you want to insert.  Here's a simple example:\n",
        "\n",
        "```\n",
        ">>> print('hello my name is %s' % 'dan')\n",
        "hello my name is dan\n",
        "```\n",
        "There are easier ways to do the simple example above, but the format we just used will come in handy later when things get more complicated.  \n",
        "\n",
        "For now, we're going to use it to allow us to sub in the name of the table we need columns names from.  \n",
        "\n",
        "Let's make a function called `get_column_names`.  This function should take a table name as an argument.  It should then insert that name into our query string that gets the columns from the information_schema.\n",
        "\n",
        "I'll first show you how it'll get added to our string, then we'll make the function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "07itCy0o2Lvc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "752a9502-5ae5-49d0-c842-52f179db685d"
      },
      "source": [
        "# First, watch what happens when you assign a table name to an object and then add it to the string\n",
        "tn = 'test_table_name'\n",
        "column_name_query =  \"\"\"SELECT column_name FROM information_schema.columns WHERE table_name = '%s' \"\"\" %tn\n",
        "column_name_query # Note how it just added the object name at the end to make a functional query"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"SELECT column_name FROM information_schema.columns WHERE table_name = 'test_table_name' \""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tLRIW6ygDR33",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "b395583b-6d90-432b-c740-c05029b26a7b"
      },
      "source": [
        "# Just for comparisons watch what happens if we didn't use %s and instead tried to insert directly.\n",
        "tn = 'test_table_name'\n",
        "column_name_query =  \"\"\"SELECT column_name FROM information_schema.columns WHERE table_name = tn \"\"\"\n",
        "column_name_query # Note how it didn't put the object 'tn' inside but just left it as a string."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'SELECT column_name FROM information_schema.columns WHERE table_name = tn '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wLnbiJfuw-jG"
      },
      "source": [
        "# Now make a function that uses this approach.\n",
        "\n",
        "def get_column_names(table_name): # arguement of table_name\n",
        "  conn, cur = get_conn_cur() # get connection and cursor\n",
        "\n",
        "  # Now select column names while inserting the table name into the WERE\n",
        "  column_name_query =  \"\"\"SELECT column_name FROM information_schema.columns\n",
        "       WHERE table_name = '%s' \"\"\" %table_name\n",
        "\n",
        "  cur.execute(column_name_query) # exectue\n",
        "  my_data = cur.fetchall() # store\n",
        "\n",
        "  cur.close() # close\n",
        "  conn.close() # close\n",
        "\n",
        "  return(my_data) # return"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lIIBumwXxgkZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7cfdf8b-6cf8-4a6c-d0d6-87fa51f8ba6a"
      },
      "source": [
        "# Test it out!\n",
        "get_column_names(table_name = 'movie_sales')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('movieid',), ('domestic',), ('international',)]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IxuzDODF2-St"
      },
      "source": [
        "Great!  These functions will be really helpful when you're joining multiple tables or doing aggregations.  They allow for you to in just one line get the info you need vs. writing a query each time.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wBV6zEjJEy4o"
      },
      "source": [
        "### Expanding our query function\n",
        "\n",
        "In last week's lesson, we had our query function where we could specify a query string and it would run it while doing all the opening, closing, etc.  One issue is that it doesn't give you the column names. That's going to be an issue later on in the lesson so let's build a bit more functionality into our `run_query` function so that it also gets the table name.  \n",
        "\n",
        "Remember that after we exectue our query our cursor stores the resulting information in memory.  We use `fetchall()` on the cursor to get that info, but the cursor also contains other info.  Specifically, the description also contains information about each column that was returned.  We can use a list comprehension to pull out the column names.  I've added to the function below!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zek-lSjAEyWb"
      },
      "source": [
        "def run_query(query_string):\n",
        "\n",
        "  conn, cur = get_conn_cur() # get connection and cursor\n",
        "\n",
        "  cur.execute(query_string) # executing string as before\n",
        "\n",
        "  my_data = cur.fetchall() # fetch query data as before\n",
        "\n",
        "  # here we're extracting the 0th element for each item in cur.description\n",
        "  colnames = [desc[0] for desc in cur.description]\n",
        "\n",
        "  cur.close() # close\n",
        "  conn.close() # close\n",
        "\n",
        "  return(colnames, my_data) # return column names AND data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pnxUj1GcJr7z"
      },
      "source": [
        "Let's just test this with a quick query.  Note how the response is a tuple of two items.  The first is a list of your column names.  The second is a list of tuples of the data you requested."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YKcfVHe5Jrbt"
      },
      "source": [
        "qs = \"\"\" SELECT * FROM movies\n",
        "            LIMIT 3; \"\"\"\n",
        "run_query(qs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v97cPWKT1O05"
      },
      "source": [
        "You could also easily store the titles and data individually as they're being returned as `colnames` and `my_data` inside the function.  Watch:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-D7LkkPu1WOP"
      },
      "source": [
        "qs = \"\"\" SELECT * FROM movies\n",
        "            LIMIT 3; \"\"\"\n",
        "titles, query_data = run_query(qs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K598MUTI1k7_"
      },
      "source": [
        "#check titles\n",
        "titles"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lydu88_x1mzm"
      },
      "source": [
        "# check data\n",
        "query_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qWATOGKa3PYu"
      },
      "source": [
        "## GROUP BY\n",
        "\n",
        "Let's now jump into doing some data aggregations.  For these you need two things.  First, your GROUP BY argument which tells SQL what level you want to group your data by.  Second, you need to apply some sort of aggregation function to a column.  Postgres has the following aggregation functions built in:\n",
        "* AVG() - Get the average of a column within the specified group.\n",
        "* COUNT() - Count how many observations are within each group.\n",
        "* MAX() - Max value within a group.\n",
        "* MIN() - Min value within a group.\n",
        "* SUM() - Sum of values in a column within a group"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J7oezmu73dCd"
      },
      "source": [
        "Let's start by just getting the average rating for each movie.  Let's first use our `get_column_names` function to remember what we're working with."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tJRf8Etz3kGD"
      },
      "source": [
        "get_column_names(table_name= 'ratings')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qs = \"\"\"SELECT * FROM ratings LIMIT 100\"\"\"\n",
        "run_query(qs)"
      ],
      "metadata": {
        "id": "8DjmXmYNT5k-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s2_wa35e3yOi"
      },
      "source": [
        "So, we want to get the average rating, so we'll need `AVG(rating)` in our `SELECT`.  And we want the average for each movie, which means we want to use `GROUP BY movieid`.  Let's try it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xzsc1tnW3cY1"
      },
      "source": [
        "# Average rating\n",
        "qs = \"\"\"SELECT movieid, AVG(rating) FROM ratings\n",
        "          GROUP BY movieid;\"\"\"\n",
        "run_query(qs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ac6iW5YE4FgS"
      },
      "source": [
        "### Aliasing and ordering\n",
        "\n",
        "That query worked great!  But there are some issues.  For one, notice how the column name for our average rating is just `'avg'`.  That's not very descriptive, but luckily you can rename or 'alias' columns.  You just write `as new_name` after the aggregating function.  Let's alias the column as `avg_rating`.\n",
        "\n",
        "Similarly, the average ratings are not in any order.  Let's add an `ORDER BY` at the end to arrange by average rating. **NOTE** we need to order by the aliased column name, so in this case `avg_rating`!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YNx58Wor6qL0"
      },
      "source": [
        "# Average rating w/ alias and order by\n",
        "qs = \"\"\"SELECT movieid, AVG(rating) as avg_rating FROM ratings\n",
        "          GROUP BY movieid\n",
        "          ORDER BY avg_rating;\"\"\"\n",
        "run_query(qs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gOKJxzuH8gvY"
      },
      "source": [
        "**TRY IT OUT**  Can you go and get a count of how many times each movie has been reviewed?  Alias the value as `number_reviews` and sort by that value."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BCS8Cj0C9L1s"
      },
      "source": [
        "# Test it out! Question 1: Write your query as the answer to the question 1.\n",
        "qs = \"\"\" ...\n",
        "          \"\"\"\n",
        "run_query(qs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ngS--6jl-gxm"
      },
      "source": [
        "### Grouping by time\n",
        "\n",
        "Just like in python, you can group by time values.  There are a couple ways to do this, but here we're going to use a built in function called `date_part()`.  This function works a lot like datetime functions in python.  You feed it two arguments.  The first is the unit of the date you want to extract from the datetime column.  The second is the datetime column.  \n",
        "\n",
        "In this case, our code will be `SELECT date_part('year', timestamp) as year, ....`.  This is saying \"grab the year part from the datetime that's inside the timestamp column and alias it as 'year'\".  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fHghM-amDiOY"
      },
      "source": [
        "# review average per year\n",
        "qs = \"\"\" SELECT date_part('year', timestamp) as year, AVG(rating) FROM ratings\n",
        "          GROUP BY year\n",
        "          ORDER BY year\n",
        "          \"\"\"\n",
        "run_query(qs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_AkewW65BdOP"
      },
      "source": [
        "## HAVING\n",
        "\n",
        "HAVING is just essentially like WHERE but instead works on your grouped results.  For example, let's get just years where the average ratings are greater than 3.5.  \n",
        "\n",
        "The only place it differs is that you call your aggregation again for the HAVING statement.  For example, we'd need `HAVING AVG(rating) >= 3.5` to filter only years that have an average >= 4.  Using the column alias we created in the select wouldn't work (e.g. `HAVING avg_rating >= 3.5`) due to order of SQL operations... more on that later.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pIpLaW9FCUmy"
      },
      "source": [
        "# Average per year where avg >= 3.5\n",
        "qs = \"\"\" SELECT date_part('year', timestamp) as year, AVG(rating) as avg_rating FROM ratings\n",
        "          GROUP BY year\n",
        "          HAVING AVG(rating) >= 3.5\n",
        "          \"\"\"\n",
        "run_query(qs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6Xur7huYEKG"
      },
      "source": [
        "**TRY IT OUT**  Get a count of how many times a movie has been reviewed.  But this time display only the movieid's and counts of movies that have been reviewed more than 50 times."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p1m7ut4ZYDNy"
      },
      "source": [
        "# Test it out! Question 2: Write your query as the answer.\n",
        "qs = \"\"\" ...\n",
        "          \"\"\"\n",
        "run_query(qs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Kzw22IWzdEn8"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0aZYfRR8qvpN"
      },
      "source": [
        "## JOIN\n",
        "\n",
        "You've already done joins in python using `.merge()`, but now we're going to do them straight away in our database.  The concept is the exactly same in that there are different types of joins, LEFT being the most common, that are connected by a key.  \n",
        "\n",
        "To do a JOIN you specify the type (LEFT JOIN, INNER JOIN, OUTER JOIN), then the dataframe you want to join, then ON followed by the columns to join on.  \n",
        "\n",
        "For example, below let's join movie sales data to our dataset of movie info.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JaLnupOYrkZG"
      },
      "source": [
        "# A quick look at the head of movies again\n",
        "qs = \"\"\"SELECT * FROM movies\n",
        "          LIMIT 5;\"\"\"\n",
        "run_query(qs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z5X9VpERsAZ_"
      },
      "source": [
        "# A quick look at the head of movie_sales\n",
        "qs = \"\"\"SELECT * FROM movie_sales\n",
        "          LIMIT 5;\"\"\"\n",
        "run_query(qs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t-WAV3LtsL1p"
      },
      "source": [
        "We want to join the sales data *to* our movie data, so the FROM will be `FROM movies` and we'll LEFT JOIN movie_sales using, well ,`LEFT JOIN movie_sales`.\n",
        "\n",
        "Note in order to tell it what columns you have your keys in you need to use the format `table_name.column_with_key`.  This is because we have the same column name, `movieid`, in both datasets so it needs to know which one to use from each."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X3TH4N6urpgl"
      },
      "source": [
        "# Join sales data to movies data\n",
        "qs = \"\"\" SELECT * FROM movies\n",
        "          LEFT JOIN movie_sales ON movies.movieid = movie_sales.movieid\n",
        "          LIMIT 3;\n",
        "          \"\"\"\n",
        "run_query(qs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-eZE4Re8tIJE"
      },
      "source": [
        "That worked!  Messy formatting aside, you can see that now we have a single return that contains the movie ID, title, genres, as well as domestic and international gross in sales.\n",
        "\n",
        "What if you wanted to attach the movie title to each individual review?  LEFT JOIN will add that info from `movies` to every row of `ratings`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bEMrUSV1tHbT"
      },
      "source": [
        "# Join sales data to movies data\n",
        "qs = \"\"\" SELECT * FROM ratings\n",
        "          LEFT JOIN movies ON ratings.movieid = movies.movieid\n",
        "          LIMIT 2;\n",
        "          \"\"\"\n",
        "run_query(qs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b-LsUHRTunxQ"
      },
      "source": [
        "### Selecting specific columns during a join\n",
        "\n",
        "Here's another order of operations thing...  even though it looks like your data isn't joined until later, that's not strictly true based on order of operations.  What this means is that you can actually select columns from both tables in your SELECT statement.  This will let you keep only what you want from both tables.  \n",
        "\n",
        "Let's run the same query as above but only get `userid, movieid, rating, timestamp, and title`. Note that just like we have to use the `table_name.column_name` format in our ON, we also need to use that in our SELECT to get just one of the `movieid` columns."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i6S2zksAune3"
      },
      "source": [
        "# Join movie data to ratings data\n",
        "qs = \"\"\" SELECT userid, ratings.movieid, rating, timestamp, title FROM ratings\n",
        "          LEFT JOIN movies ON ratings.movieid = movies.movieid\n",
        "          LIMIT 2;\n",
        "          \"\"\"\n",
        "run_query(qs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TXa2-_A8wcHV"
      },
      "source": [
        "**TRY IT OUT**  Just do a simple left join bringing the sales table on to the reviews table."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HObY6Ma0wu97"
      },
      "source": [
        "# Join sales data to ratings data\n",
        "qs = \"\"\"\n",
        "          \"\"\"\n",
        "run_query(qs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C6mjIKS464ed"
      },
      "source": [
        "## Subqueries\n",
        "\n",
        "In python you would commonly filter results by several elements in different columns.  For example, if you had a bunch of spotify data in a single table you might filter by artist name and tempo and number of listens.  It makes sense to do the same thing here in SQL land, but it's a touch trickier as often those data will be in different tables.  So, it makes it difficult if you want to retrieve data from one table that meets a condition that's in another table.  \n",
        "\n",
        "For example, what if we want to get only the movie ratings for movies that are a comedy.  Ratings are in one table, `ratings`, but we want to filter by genre which is over in the `movies` table.  To do this we need to use a subquery.  Essentially we'll filter via WHERE `ratings` by the output of another query on `movies`.\n",
        "\n",
        "Let me show you the whole thing and then I'll break it down a bit more.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zJ0opg2y7CiP"
      },
      "source": [
        "qs = \"\"\" SELECT * FROM ratings\n",
        "            WHERE movieid IN (SELECT movieid FROM movies WHERE genres LIKE '%Comedy%')\n",
        "            LIMIT 20;\"\"\"\n",
        "run_query(qs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qs = \"\"\" SELECT movieid, rating, to_char(timestamp, 'MM-DD-YYYY') FROM ratings\n",
        "            WHERE movieid IN (SELECT movieid FROM movies WHERE genres LIKE '%Comedy%')\n",
        "            LIMIT 20;\"\"\"\n",
        "run_query(qs)"
      ],
      "metadata": {
        "id": "jEWREjBPdqaX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6pjB6ahGEAwT"
      },
      "source": [
        "I limited the return so it wasn't overly long. But, you can see that only a few of the movieid's are represented.  In this case, only those that are comedies.  \n",
        "\n",
        "Let's take a second and run the subquery to show you what it would return."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pS1bI_ZL7Fw7"
      },
      "source": [
        "qs = \"\"\" SELECT movieid FROM movies\n",
        "            WHERE genres LIKE '%Comedy%' \"\"\"\n",
        "run_query(qs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GCg5_VVeEdgt"
      },
      "source": [
        "See, it gives us just those ID's where 'Comedy' appears in the genres column.  You could in theory hardcode this without the subquery.  For example, you could have used the following:\n",
        "\n",
        "```\n",
        "qs = \"\"\" SELECT * FROM ratings\n",
        "            WHERE movieid IN (1, 3, 4, 5, 7);\"\"\"\n",
        "```\n",
        "\n",
        "The obvious problem here is that this isn't dynamic.  What if you want a different genre?  You'd have to go and first do a query in `movies` to find out all the ID's that match, then hardcode them in this second query.  And if your data changes you'd have to update it.  Or, if your data are even remotely long it's just not possible to hardcode.  Remember that this full dataset has 67k movies, of which likely 1/4 are comedies.  You can't reasonably hardcode that so you *must* use a subquery!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GmRFLNgQITHq"
      },
      "source": [
        "**TRY IT OUT**  Can you write a query that gets the titles of movies that made over 99 million dollars in domestic sales? (the numbers of not real, so don't get surprised :) )\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LeGNC8LLIPKM"
      },
      "source": [
        "#Question 3: Write your query as the answer\n",
        "qs = \"\"\" ...\n",
        "          \"\"\"\n",
        "run_query(qs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-4Meky_ZNSsx"
      },
      "source": [
        "Of course, you can have a more complicated subquery.  For example what if we want to get just the titles of movies where the average rating is 3.5 or greater.  We'll need to do a GROUP BY in our subquery.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HJEPgyJVNOP-"
      },
      "source": [
        "qs = \"\"\" SELECT title FROM movies\n",
        "            WHERE movieid IN (SELECT movieid FROM ratings\n",
        "                                GROUP By movieid\n",
        "                                HAVING AVG(rating) >= 3.5) \"\"\"\n",
        "run_query(qs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_23FZx6pdVv3"
      },
      "source": [
        "# Feature generation\n",
        "\n",
        "The last thing I'm going to show you is just that you can do more manipulations of columns in your select.  Specifically, you can do math with columns in SELECT. Obviously you can do this rapidly in python, but why bring in a bunch of columns to do simple math on when you can bring in just the resulting one.  \n",
        "\n",
        "For example, below I get the total_sales by adding the domestic and international columns. I also alias it as 'total_sales'.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "th6lGadZdXgG"
      },
      "source": [
        "# Add two columns plus an alias\n",
        "qs = \"\"\" SELECT movieid, (domestic+international) as total_sales FROM movie_sales;\n",
        "          \"\"\"\n",
        "run_query(qs)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}